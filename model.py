import os
import keras_tuner as kt
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
from tensorflow.keras.layers import Conv2D, Dense, Dropout, Flatten, Lambda
from tensorflow.keras.models import Sequential
from tensorflow.keras.optimizers import Adam, RMSprop
import utils

DATA_DIR = "./data"
TEST_SIZE = 0.2
EPOCHS_SEARCH = 10
EPOCHS_FINAL = 50
INPUT_SHAPE = (utils.IMAGE_HEIGHT, utils.IMAGE_WIDTH, utils.IMAGE_CHANNELS)
MODEL_DIR = "model"
os.makedirs(MODEL_DIR, exist_ok=True)


def load_data():
    """
    Folder structure of data should be:
    ./data
    -- Left
    -- Right
    -- Center
    -- drive_log.csv, generated by the label generator script
    """
    csv_path = os.path.join(
        DATA_DIR, "driving_log.csv"
    )  # consist of two cols, first is path and second is steering angle
    data = pd.read_csv(csv_path)
    image_paths = []
    steering_angles = []

    for _, row in data.iterrows():
        img_path = os.path.join(DATA_DIR, row["image_path"])
        angle = float(row["steering_angle"])
        image_paths.append(img_path)
        steering_angles.append(angle)

    return np.array(image_paths), np.array(steering_angles)

# architecture reference from: https://developer.nvidia.com/blog/deep-learning-self-driving-cars/
def build_model(hp):
    """
    Tunes:
    -- Conv filters
    -- dense layer
    -- dropout rate
    -- learning rate of the model
    -- optimizer (adam and rmsprop)
    """
    model = Sequential()
    # normalize
    model.add(Lambda(lambda x: x / 127.5 - 1.0, input_shape=INPUT_SHAPE))
    act_func = "elu"  # activation function
    model.add(
        Conv2D(
            hp.Int("conv_1", 16, 32, step=8, default=24),
            (5, 5),
            strides=(2, 2),
            activation=act_func,
        )
    )
    model.add(
        Conv2D(
            hp.Int("conv_2", 24, 48, step=12, default=36),
            (5, 5),
            strides=(2, 2),
            activation=act_fuc,
        )
    )
    model.add(
        Conv2D(
            hp.Int("conv_3", 36, 64, step=12, default=48),
            (5, 5),
            strides=(2, 2),
            activation=act_func,
        )
    )
    model.add(Conv2D(64, (3, 3), activation=act_func))
    model.add(Conv2D(64, (3, 3), activation=act_func))
    dropout_rate = hp.Float("dropout", 0.2, 0.5, step=0.1, default=0.4)
    model.add(Dropout(dropout_rate))
    model.add(Flatten())
    model.add(
        Dense(hp.Int("dense_1", 50, 120, step=20, default=100), activation=act_func)
    )
    model.add(Dense(50, activation=act_func))
    model.add(Dense(10, activation=act_func))
    model.add(Dense(1))

    lr = hp.Float("learning_rate", 1e-4, 1e-3, sampling="log", default=1e-3)

    # choose optimizer
    if hp.Choice("optimizer", values=["adam", "rmsprop"]) == "adam":
        optimizer = Adam(learning_rate=lr)
    else:
        optimizer = RMSprop(learning_rate=lr)

    model.compile(loss="mse", optimizer=optimizer)
    return model


# since training data is very less for the large NN, so use parameter tunning and large argumentation
class MyHyperTuner(kt.BayesianOptimization):
    def run_trial(self, trial, *args, **kwargs):
        hp = trial.hyperparameters
        batch_size = hp.Int("batch_size", 2, 16, step=2)  # batch size range for tunning

        # access data from kwargs
        X_t = kwargs["x_train"]
        y_t = kwargs["y_train"]
        X_v = kwargs["x_valid"]
        y_v = kwargs["y_valid"]

        train_gen = utils.batch_generator(DATA_DIR, X_t, y_t, batch_size, True)
        valid_gen = utils.batch_generator(DATA_DIR, X_v, y_v, batch_size, False)

        # standard steps calc
        steps_per_epoch = len(X_t) // batch_size
        validation_steps = len(X_v) // batch_size

        model = self.hypermodel.build(hp)

        # remove custom kwargs before passing to fit
        fit_kwargs = {
            k: v
            for k, v in kwargs.items()
            if k not in ["x_train", "y_train", "x_valid", "y_valid"]
        }

        return model.fit(
            train_gen,
            steps_per_epoch=steps_per_epoch,
            validation_data=valid_gen,
            validation_steps=validation_steps,
            **fit_kwargs,
        )


if __name__ == "__main__":
    X, y = load_data()
    X_train, X_valid, y_train, y_valid = train_test_split(
        X, y, test_size=TEST_SIZE, random_state=42
    )

    tuner = MyHyperTuner(
        build_model,
        objective="val_loss",
        max_trials=10,
        executions_per_trial=1,
        directory="tuning_dir",
        project_name="advanced_tuning",
    )

    # pass data explicitly to the search function
    tuner.search(
        x_train=X_train,
        y_train=y_train,
        x_valid=X_valid,
        y_valid=y_valid,
        epochs=EPOCHS_SEARCH,
        callbacks=[EarlyStopping(monitor="val_loss", patience=3)],
    )

    best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]
    best_batch_size = best_hps.get("batch_size")

    print(f"Best Batch Size: {best_batch_size}")

    # final traing on best found hyper parameters
    model = tuner.hypermodel.build(best_hps)

    train_gen = utils.batch_generator(DATA_DIR, X_train, y_train, best_batch_size, True)
    valid_gen = utils.batch_generator(
        DATA_DIR, X_valid, y_valid, best_batch_size, False
    )

    # saving the best model based on the validation loss
    checkpoint = ModelCheckpoint(
        filepath=os.path.join(MODEL_DIR, "model_best.h5"),
        monitor="val_loss",
        save_best_only=True,
        verbose=1
        )

    model.fit(
        train_gen,
        steps_per_epoch=len(X_train) // best_batch_size,
        validation_data=valid_gen,
        validation_steps=len(X_valid) // best_batch_size,
        epochs=EPOCHS_FINAL,
        callbacks=[checkpoint],
        verbose=1,
        )

    model.save(os.path.join(MODEL_DIR, "model_final.h5"))

# contributed by both
# took time to get the good performing hyperparameter
